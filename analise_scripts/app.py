import streamlit as st
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import altair as alt

# Configura√ß√£o da P√°gina
st.set_page_config(page_title="AI Moderator", page_icon="üõ°Ô∏è", layout="wide")

# Usamos @st.cache para carregar o modelo s√≥ uma vez e n√£o ficar lento
@st.cache_resource
def load_model():
    model_name = "unitary/toxic-bert"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    return tokenizer, model

tokenizer, model = load_model()

def analyze_text(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.sigmoid(outputs.logits).numpy()[0]
    return probs


st.title("üõ°Ô∏è AI Content Moderator: YouTube vs Reddit")
st.markdown("""
Esta aplica√ß√£o utiliza um **LLM (Toxic-BERT)** para analisar toxicidade em redes sociais 
e auxiliar moderadores humanos na dete√ß√£o de discurso de √≥dio.
""")

tab1, tab2 = st.tabs(["üìä An√°lise Comparativa (Dashboard)", "‚ö° Teste em Tempo Real"])

with tab1:
    st.header("O Problema: Toxicidade nas redes sociais")
    
    # Carregar os teus CSVs organizados
    try:
        df_yt = pd.read_csv("csv_analise_final_organizados/resultado_analise_youtube_organizado.csv")
        df_rd = pd.read_csv("csv_analise_final_organizados/resultado_analise_reddit_organizado.csv")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("YouTube")
            # Calculo da percentagem √≥dio de identidade
            odio_yt = len(df_yt[df_yt['Categoria_Final'].str.contains("√ìdio de Identidade")])
            pct_yt = (odio_yt / len(df_yt)) * 100
            st.metric("Taxa de √ìdio de Identidade", f"{pct_yt:.2f}%")
            st.dataframe(df_yt[['text', 'Categoria_Final']].head(10), use_container_width=True)
            
        with col2:
            st.subheader("Reddit")
            odio_rd = len(df_rd[df_rd['Categoria_Final'].str.contains("√ìdio de Identidade")])
            pct_rd = (odio_rd / len(df_rd)) * 100

            st.metric("Taxa de √ìdio de Identidade", f"{pct_rd:.2f}%")
            
            st.dataframe(df_rd[['text', 'Categoria_Final']].head(10), use_container_width=True)

        st.markdown("---")
        st.subheader("Visualiza√ß√£o Gr√°fica")
        
        #Preparar os dados para o gr√°fico
        df_chart = pd.concat([df_yt.assign(Plataforma='YouTube'), df_rd.assign(Plataforma='Reddit')])
        
        chart_data = df_chart.groupby(['Categoria_Final', 'Plataforma']).size().reset_index(name='Contagem')
        
        ordem_categorias = [
            "√ìdio de Identidade (Racismo/Xenofobia/etc)",
            "Amea√ßa Violenta",
            "Toxicidade Extrema",
            "Insulto",
            "T√≥xico Geral",
            "Neutro/Seguro"
        ]
        
        chart = alt.Chart(chart_data).mark_bar().encode(
            x=alt.X('Contagem', title='N√∫mero de Coment√°rios'),
            y=alt.Y('Categoria_Final', sort=ordem_categorias, title=None),
            color=alt.Color('Plataforma', 
                            scale=alt.Scale(domain=['YouTube', 'Reddit'], range=['#FF0000', '#FF4500']),
                            legend=alt.Legend(title="Plataforma")),
            tooltip=['Plataforma', 'Categoria_Final', 'Contagem'], 
            yOffset='Plataforma' 
        ).properties(
            height=500,
            title="Compara√ß√£o de Toxicidade por Categoria"
        ).configure_axis(
            labelFontSize=12,
            titleFontSize=14
        )
        
        # Mostrar o gr√°fico no Streamlit
        st.altair_chart(chart, use_container_width=True)

    except FileNotFoundError:
        st.error("‚ö†Ô∏è Ficheiros CSV 'organizados' n√£o encontrados. Por favor execute os scripts de an√°lise primeiro.")

with tab2:
    st.header("Simulador de Modera√ß√£o Autom√°tica")
    st.write("Teste o modelo de LLM com qualquer frase (em Ingl√™s) para ver como ele classifica.")

    user_input = st.text_area("Digite um coment√°rio para analisar:", "You are stupid but I respect your opinion.")
    
    if st.button("Analisar Coment√°rio"):
        if user_input:
            with st.spinner("O BERT est√° a pensar..."):
                scores = analyze_text(user_input)
                
            labels = ["T√≥xico", "Muito T√≥xico", "Obsceno", "Amea√ßa", "Insulto", "√ìdio de Identidade"]
            
            col_a, col_b = st.columns(2)
            
            with col_a:
                st.subheader("Diagn√≥stico")
                max_score = max(scores)
                if scores[5] > 0.5: # Identity Hate
                    st.error("üö® BLOQUEIO IMEDIATO: Discurso de √ìdio detetado.")
                elif max_score > 0.5:
                    st.warning("‚ö†Ô∏è REVIS√ÉO NECESS√ÅRIA: Conte√∫do t√≥xico.")
                else:
                    st.success("‚úÖ APROVADO: Coment√°rio seguro.")
            
            with col_b:
                st.subheader("Detalhes do Modelo")
                for label, score in zip(labels, scores):
                    st.progress(float(score), text=f"{label}: {score:.1%}")